#! /usr/bin/env python

from csv import DictReader
from pathlib import Path
from datetime import datetime
from argparse import ArgumentParser
from sys import stdin, exit
from subprocess import run
from os import cpu_count, environ

from requests import post


def parse_args():
    """
    Parse the CLI args
    """
    parser = ArgumentParser()
    subparsers = parser.add_subparsers(help="Sub-command help", dest="subcommand")

    parser_collect_csv = subparsers.add_parser("collect-csv", help="Collect csv files and place them in their right directories")
    parser_collect_csv.add_argument("--dest-dir", help="Output dir to store the results", default="output_dir")
    parser_collect_csv.add_argument("--search-paths", nargs='*', default=["."], help="List of paths to recursively search for files")

    parser_backup_tables = subparsers.add_parser("backup-tables", help="Make a pg_dump of the selected tables")
    parser_backup_tables.add_argument("database", help="Name of the database to make the backup")
    parser_backup_tables.add_argument("tables", nargs='*', help="List of tables to dump", default="-")
    parser_backup_tables.add_argument("--outfile", help="Filename for the backup", default="backup.sql")

    return parser.parse_args()


def collect_csv(source_dir, dest_dir):
    """
    Scan for csvs in a source_dir recursively.

    Place each file in a path of <dest_dir>/<species>/<YYYYMMDDHHMM>.csv
    """
    source_dir = Path(source_dir)
    dest_dir = Path(dest_dir)
    for csvfile in source_dir.rglob("*.csv"):
        species = normalized_species(csvfile)
        species_dir = dest_dir / species
        species_dir.mkdir(exist_ok=True, parents=True)
        date_time = normalized_datetime(csvfile)
        print(f"Renaming {csvfile} to {species_dir / (date_time + '.csv')}")
        csvfile.rename(species_dir / (date_time + ".csv"))


def normalized_species(csv_filename):
    """
    Return the species of the column "species" of the csv_filename.

    Normalize it via lowercasing and transforming spaces into "_"
    """
    with open(csv_filename) as csvfilename:
        reader = DictReader(csvfilename)
        first_row = next(reader)
        return first_row.get("species").lower().replace(" ", "_")


def normalized_datetime(csv_filename):
    """
    Return the datetime of the column "observation_date" of the csv_filename.

    Normalize it with the format YYYYMMDDHHMM
    """
    with open(csv_filename) as csvfilename:
        reader = DictReader(csvfilename)
        first_row = next(reader)
        src_date_fmt = "%d/%m/%Y %H:%M"
        dst_date_fmt = "%Y%m%d%H%M"
        obs_date = datetime.strptime(first_row.get("observation_date"), src_date_fmt)
        return obs_date.strftime(dst_date_fmt)


def backup_tables(tables, backup_filename):
    """
    Backup a list of tables using pg_dump to a backup_filename

    Notify via Slack in case of failure
    """
    tables_switches = " ".join(f"-t {table}" for table in tables)
    jobs = cpu_count()
    cmd = f"pg_dump {tables_switches} -j {jobs} -Fc > {backup_filename}"
    pg_dump = run(cmd, shell=True, capture_output=True)
    if pg_dump.returncode != 0:
        webhook_url = environ.get("SLACK_WEBHOOK_URL")
        if webhook_url:
            msg = "Failed to {cmd}:\n{pg_dump.stderr.decode()}"
            notify_via_slack(webhook_url, msg)
        exit(pg_dump.returncode)


def notify_via_slack(webhook_url, msg):
    """
    Notify via Slack webhook url
    """
    slack_data = {"text": msg}
    post(webhook_url, json=slack_data)


if __name__ == "__main__":

    args = parse_args()

    if args.subcommand == "collect-csv":
        for path in args.search_paths:
            collect_csv(path, args.dest_dir)

    elif args.subcommand == "backup-tables":
        if args.tables == "-":
            tables = stdin.read().split()
        else:
            tables = args.tables
        backup_tables(tables, args.outfile)
